{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e948f64f",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-11-01T13:20:17.051466Z",
          "iopub.status.busy": "2024-11-01T13:20:17.050716Z",
          "iopub.status.idle": "2024-11-01T13:20:17.923627Z",
          "shell.execute_reply": "2024-11-01T13:20:17.922598Z"
        },
        "papermill": {
          "duration": 0.882094,
          "end_time": "2024-11-01T13:20:17.926102",
          "exception": false,
          "start_time": "2024-11-01T13:20:17.044008",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e948f64f",
        "outputId": "7ce7131b-86d7-45ef-af33-0431c927ffb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['image_99.jpg', 'image_35.jpg', 'image_0.jpg', 'image_31.jpg', 'image_60.jpg', 'image_59.jpg', 'image_70.jpg', 'image_19.jpg', 'image_28.jpg', 'image_13.jpg', 'image_18.jpg', 'image_32.jpg', 'image_46.jpg', 'image_33.jpg', 'image_30.jpg', 'image_81.jpg', 'image_24.jpg', 'image_37.jpg', 'image_7.jpg', 'image_65.jpg', 'image_79.jpg', 'image_36.jpg', 'image_43.jpg', 'image_87.jpg', 'image_10.jpg', 'image_74.jpg', 'image_54.jpg', 'image_26.jpg', 'image_17.jpg', 'image_63.jpg', 'image_97.jpg', 'image_29.jpg', 'image_78.jpg', 'image_8.jpg', 'image_56.jpg', 'image_6.jpg', 'image_3.jpg', 'image_45.jpg', 'image_11.jpg', 'image_44.jpg', 'image_95.jpg', 'image_50.jpg', 'image_93.jpg', 'image_82.jpg', 'image_42.jpg', 'image_64.jpg', 'image_77.jpg', 'image_69.jpg', 'image_15.jpg', 'image_52.jpg', 'image_2.jpg', 'image_89.jpg', 'image_71.jpg', 'image_57.jpg', 'image_85.jpg', 'image_72.jpg', 'image_58.jpg', 'image_20.jpg', 'image_9.jpg', 'image_91.jpg', 'image_38.jpg', 'image_84.jpg', 'image_5.jpg', 'image_75.jpg', 'image_92.jpg', 'image_39.jpg', 'image_53.jpg', 'image_76.jpg', 'image_51.jpg', 'image_55.jpg', 'image_94.jpg', 'image_1.jpg', 'image_21.jpg', 'image_62.jpg', 'image_23.jpg', 'image_48.jpg', 'image_68.jpg', 'image_47.jpg', 'image_16.jpg', 'image_80.jpg', 'image_22.jpg', 'image_34.jpg', 'image_49.jpg', 'image_83.jpg', 'image_90.jpg', 'image_4.jpg', 'image_86.jpg', 'image_14.jpg', 'image_25.jpg', 'image_96.jpg', 'image_27.jpg', 'image_41.jpg', 'image_98.jpg', 'image_73.jpg', 'image_67.jpg', 'image_88.jpg', 'image_61.jpg', 'image_66.jpg', 'image_40.jpg', 'image_12.jpg']\n",
            "['image_67.jpg', 'image_70.jpg', 'image_20.jpg', 'image_72.jpg', 'image_16.jpg', 'image_5.jpg', 'image_10.jpg', 'image_23.jpg', 'image_81.jpg', 'image_49.jpg', 'image_12.jpg', 'image_86.jpg', 'image_11.jpg', 'image_69.jpg', 'image_1.jpg', 'image_50.jpg', 'image_29.jpg', 'image_39.jpg', 'image_84.jpg', 'image_53.jpg', 'image_51.jpg', 'image_74.jpg', 'image_56.jpg', 'image_26.jpg', 'image_54.jpg', 'image_59.jpg', 'image_0.jpg', 'image_66.jpg', 'image_47.jpg', 'image_3.jpg', 'image_40.jpg', 'image_30.jpg', 'image_34.jpg', 'image_71.jpg', 'image_79.jpg', 'image_41.jpg', 'image_57.jpg', 'image_42.jpg', 'image_27.jpg', 'image_13.jpg', 'image_24.jpg', 'image_8.jpg', 'image_35.jpg', 'image_18.jpg', 'image_38.jpg', 'image_19.jpg', 'image_65.jpg', 'image_83.jpg', 'image_43.jpg', 'image_2.jpg', 'image_85.jpg', 'image_68.jpg', 'image_37.jpg', 'image_22.jpg', 'image_28.jpg', 'image_76.jpg', 'image_33.jpg', 'image_6.jpg', 'image_75.jpg', 'image_17.jpg', 'image_73.jpg', 'image_14.jpg', 'image_52.jpg', 'image_80.jpg', 'image_45.jpg', 'image_36.jpg', 'image_60.jpg', 'image_62.jpg', 'image_58.jpg', 'image_4.jpg', 'image_31.jpg', 'image_46.jpg', 'image_64.jpg', 'image_78.jpg', 'image_77.jpg', 'image_44.jpg', 'image_15.jpg', 'image_55.jpg', 'image_63.jpg', 'image_7.jpg', 'image_25.jpg', 'image_61.jpg', 'image_48.jpg', 'image_21.jpg', 'image_32.jpg', 'image_82.jpg', 'image_99.jpg', 'image_95.jpg', 'image_93.jpg', 'image_94.jpg', 'image_90.jpg', 'image_9.jpg', 'image_91.jpg', 'image_87.jpg', 'image_92.jpg', 'image_89.jpg', 'image_96.jpg', 'image_88.jpg', 'image_98.jpg', 'image_97.jpg']\n",
            "['image_1.jpg', 'image_0.jpg', 'image_10.jpg', 'image_16.jpg', 'image_20.jpg', 'image_17.jpg', 'image_15.jpg', 'image_2.jpg', 'image_22.jpg', 'image_19.jpg', 'image_13.jpg', 'image_11.jpg', 'image_12.jpg', 'image_18.jpg', 'image_14.jpg', 'image_21.jpg', 'image_24.jpg', 'image_27.jpg', 'image_25.jpg', 'image_33.jpg', 'image_34.jpg', 'image_30.jpg', 'image_31.jpg', 'image_23.jpg', 'image_29.jpg', 'image_28.jpg', 'image_3.jpg', 'image_32.jpg', 'image_26.jpg', 'image_38.jpg', 'image_35.jpg', 'image_43.jpg', 'image_37.jpg', 'image_40.jpg', 'image_39.jpg', 'image_41.jpg', 'image_46.jpg', 'image_47.jpg', 'image_45.jpg', 'image_36.jpg', 'image_44.jpg', 'image_42.jpg', 'image_4.jpg', 'image_56.jpg', 'image_52.jpg', 'image_49.jpg', 'image_53.jpg', 'image_58.jpg', 'image_55.jpg', 'image_54.jpg', 'image_50.jpg', 'image_59.jpg', 'image_5.jpg', 'image_51.jpg', 'image_48.jpg', 'image_57.jpg', 'image_67.jpg', 'image_66.jpg', 'image_7.jpg', 'image_71.jpg', 'image_62.jpg', 'image_68.jpg', 'image_70.jpg', 'image_60.jpg', 'image_61.jpg', 'image_6.jpg', 'image_64.jpg', 'image_65.jpg', 'image_69.jpg', 'image_63.jpg', 'image_77.jpg', 'image_8.jpg', 'image_72.jpg', 'image_74.jpg', 'image_81.jpg', 'image_76.jpg', 'image_73.jpg', 'image_75.jpg', 'image_79.jpg', 'image_78.jpg', 'image_82.jpg', 'image_80.jpg', 'image_83.jpg', 'image_86.jpg', 'image_9.jpg', 'image_84.jpg', 'image_88.jpg', 'image_92.jpg', 'image_89.jpg', 'image_90.jpg', 'image_91.jpg', 'image_87.jpg', 'image_85.jpg', 'image_93.jpg', 'image_94.jpg', 'image_95.jpg', 'image_99.jpg', 'image_98.jpg', 'image_97.jpg', 'image_96.jpg']\n",
            "['image_99.jpg', 'image_35.jpg', 'image_0.jpg', 'image_31.jpg', 'image_60.jpg', 'image_59.jpg', 'image_70.jpg', 'image_19.jpg', 'image_28.jpg', 'image_13.jpg', 'image_18.jpg', 'image_32.jpg', 'image_46.jpg', 'image_33.jpg', 'image_30.jpg', 'image_81.jpg', 'image_24.jpg', 'image_37.jpg', 'image_7.jpg', 'image_65.jpg', 'image_79.jpg', 'image_36.jpg', 'image_43.jpg', 'image_87.jpg', 'image_10.jpg', 'image_74.jpg', 'image_54.jpg', 'image_26.jpg', 'image_17.jpg', 'image_63.jpg', 'image_97.jpg', 'image_29.jpg', 'image_78.jpg', 'image_8.jpg', 'image_56.jpg', 'image_6.jpg', 'image_3.jpg', 'image_45.jpg', 'image_11.jpg', 'image_44.jpg', 'image_95.jpg', 'image_50.jpg', 'image_93.jpg', 'image_82.jpg', 'image_42.jpg', 'image_64.jpg', 'image_77.jpg', 'image_69.jpg', 'image_15.jpg', 'image_52.jpg', 'image_2.jpg', 'image_89.jpg', 'image_71.jpg', 'image_57.jpg', 'image_85.jpg', 'image_72.jpg', 'image_58.jpg', 'image_20.jpg', 'image_9.jpg', 'image_91.jpg', 'image_38.jpg', 'image_84.jpg', 'image_5.jpg', 'image_75.jpg', 'image_92.jpg', 'image_39.jpg', 'image_53.jpg', 'image_76.jpg', 'image_51.jpg', 'image_55.jpg', 'image_94.jpg', 'image_1.jpg', 'image_21.jpg', 'image_62.jpg', 'image_23.jpg', 'image_48.jpg', 'image_68.jpg', 'image_47.jpg', 'image_16.jpg', 'image_80.jpg', 'image_22.jpg', 'image_34.jpg', 'image_49.jpg', 'image_83.jpg', 'image_90.jpg', 'image_4.jpg', 'image_86.jpg', 'image_14.jpg', 'image_25.jpg', 'image_96.jpg', 'image_27.jpg', 'image_41.jpg', 'image_98.jpg', 'image_73.jpg', 'image_67.jpg', 'image_88.jpg', 'image_61.jpg', 'image_66.jpg', 'image_40.jpg', 'image_12.jpg']\n",
            "['image_67.jpg', 'image_70.jpg', 'image_20.jpg', 'image_72.jpg', 'image_16.jpg', 'image_5.jpg', 'image_10.jpg', 'image_23.jpg', 'image_81.jpg', 'image_49.jpg', 'image_12.jpg', 'image_86.jpg', 'image_11.jpg', 'image_69.jpg', 'image_1.jpg', 'image_50.jpg', 'image_29.jpg', 'image_39.jpg', 'image_84.jpg', 'image_53.jpg', 'image_51.jpg', 'image_74.jpg', 'image_56.jpg', 'image_26.jpg', 'image_54.jpg', 'image_59.jpg', 'image_0.jpg', 'image_66.jpg', 'image_47.jpg', 'image_3.jpg', 'image_40.jpg', 'image_30.jpg', 'image_34.jpg', 'image_71.jpg', 'image_79.jpg', 'image_41.jpg', 'image_57.jpg', 'image_42.jpg', 'image_27.jpg', 'image_13.jpg', 'image_24.jpg', 'image_8.jpg', 'image_35.jpg', 'image_18.jpg', 'image_38.jpg', 'image_19.jpg', 'image_65.jpg', 'image_83.jpg', 'image_43.jpg', 'image_2.jpg', 'image_85.jpg', 'image_68.jpg', 'image_37.jpg', 'image_22.jpg', 'image_28.jpg', 'image_76.jpg', 'image_33.jpg', 'image_6.jpg', 'image_75.jpg', 'image_17.jpg', 'image_73.jpg', 'image_14.jpg', 'image_52.jpg', 'image_80.jpg', 'image_45.jpg', 'image_36.jpg', 'image_60.jpg', 'image_62.jpg', 'image_58.jpg', 'image_4.jpg', 'image_31.jpg', 'image_46.jpg', 'image_64.jpg', 'image_78.jpg', 'image_77.jpg', 'image_44.jpg', 'image_15.jpg', 'image_55.jpg', 'image_63.jpg', 'image_7.jpg', 'image_25.jpg', 'image_61.jpg', 'image_48.jpg', 'image_21.jpg', 'image_32.jpg', 'image_82.jpg', 'image_99.jpg', 'image_95.jpg', 'image_93.jpg', 'image_94.jpg', 'image_90.jpg', 'image_9.jpg', 'image_91.jpg', 'image_87.jpg', 'image_92.jpg', 'image_89.jpg', 'image_96.jpg', 'image_88.jpg', 'image_98.jpg', 'image_97.jpg']\n",
            "['image_1.jpg', 'image_0.jpg', 'image_10.jpg', 'image_16.jpg', 'image_20.jpg', 'image_17.jpg', 'image_15.jpg', 'image_2.jpg', 'image_22.jpg', 'image_19.jpg', 'image_13.jpg', 'image_11.jpg', 'image_12.jpg', 'image_18.jpg', 'image_14.jpg', 'image_21.jpg', 'image_24.jpg', 'image_27.jpg', 'image_25.jpg', 'image_33.jpg', 'image_34.jpg', 'image_30.jpg', 'image_31.jpg', 'image_23.jpg', 'image_29.jpg', 'image_28.jpg', 'image_3.jpg', 'image_32.jpg', 'image_26.jpg', 'image_38.jpg', 'image_35.jpg', 'image_43.jpg', 'image_37.jpg', 'image_40.jpg', 'image_39.jpg', 'image_41.jpg', 'image_46.jpg', 'image_47.jpg', 'image_45.jpg', 'image_36.jpg', 'image_44.jpg', 'image_42.jpg', 'image_4.jpg', 'image_56.jpg', 'image_52.jpg', 'image_49.jpg', 'image_53.jpg', 'image_58.jpg', 'image_55.jpg', 'image_54.jpg', 'image_50.jpg', 'image_59.jpg', 'image_5.jpg', 'image_51.jpg', 'image_48.jpg', 'image_57.jpg', 'image_67.jpg', 'image_66.jpg', 'image_7.jpg', 'image_71.jpg', 'image_62.jpg', 'image_68.jpg', 'image_70.jpg', 'image_60.jpg', 'image_61.jpg', 'image_6.jpg', 'image_64.jpg', 'image_65.jpg', 'image_69.jpg', 'image_63.jpg', 'image_77.jpg', 'image_8.jpg', 'image_72.jpg', 'image_74.jpg', 'image_81.jpg', 'image_76.jpg', 'image_73.jpg', 'image_75.jpg', 'image_79.jpg', 'image_78.jpg', 'image_82.jpg', 'image_80.jpg', 'image_83.jpg', 'image_86.jpg', 'image_9.jpg', 'image_84.jpg', 'image_88.jpg', 'image_92.jpg', 'image_89.jpg', 'image_90.jpg', 'image_91.jpg', 'image_87.jpg', 'image_85.jpg', 'image_93.jpg', 'image_94.jpg', 'image_95.jpg', 'image_99.jpg', 'image_98.jpg', 'image_97.jpg', 'image_96.jpg']\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/FIRE-SMOKE-DATASET/Test'):\n",
        "    # print(filenames)\n",
        "    for drn,_,fln in os.walk(dirname):\n",
        "        print(fln)\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpJ5feM6Vn14",
        "outputId": "50d0690d-6ad6-42c3-b23f-25fbba804ce1"
      },
      "id": "JpJ5feM6Vn14",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b8ffa3",
      "metadata": {
        "papermill": {
          "duration": 0.004318,
          "end_time": "2024-11-01T13:20:17.935392",
          "exception": false,
          "start_time": "2024-11-01T13:20:17.931074",
          "status": "completed"
        },
        "tags": [],
        "id": "99b8ffa3"
      },
      "source": [
        "1. Load the Pre-trained CNN:\n",
        "    \n",
        "    1.1. Import a CNN (e.g., VGG16, ResNet50) from a deep learning library like TensorFlow or PyTorch.\n",
        "\n",
        "    1.2. Remove the top classification layer of the network to get the feature map output instead. For instance, VGG16 outputs a 4096-dimensional feature vector from its fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18318723",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-01T13:20:17.946580Z",
          "iopub.status.busy": "2024-11-01T13:20:17.946056Z",
          "iopub.status.idle": "2024-11-01T13:20:32.094106Z",
          "shell.execute_reply": "2024-11-01T13:20:32.093271Z"
        },
        "papermill": {
          "duration": 14.156578,
          "end_time": "2024-11-01T13:20:32.096575",
          "exception": false,
          "start_time": "2024-11-01T13:20:17.939997",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18318723",
        "outputId": "af56645d-97ea-4398-c461-2b17bc9f8342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained VGG16 model and exclude the final classification layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149d620c",
      "metadata": {
        "papermill": {
          "duration": 0.00523,
          "end_time": "2024-11-01T13:20:32.107263",
          "exception": false,
          "start_time": "2024-11-01T13:20:32.102033",
          "status": "completed"
        },
        "tags": [],
        "id": "149d620c"
      },
      "source": [
        "2. Extract Features:\n",
        "\n",
        "    2.1 Preprocess each image (resize, normalize) to fit the input requirements of the CNN model.\n",
        "   \n",
        "    2.2 Pass each image through the feature extractor and flatten the output to create a vector of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d3727b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-01T13:20:32.120947Z",
          "iopub.status.busy": "2024-11-01T13:20:32.120161Z",
          "iopub.status.idle": "2024-11-01T13:20:32.129237Z",
          "shell.execute_reply": "2024-11-01T13:20:32.128471Z"
        },
        "papermill": {
          "duration": 0.018811,
          "end_time": "2024-11-01T13:20:32.131341",
          "exception": false,
          "start_time": "2024-11-01T13:20:32.112530",
          "status": "completed"
        },
        "tags": [],
        "id": "4d3727b3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "def preprocess_and_extract(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Extract features\n",
        "    features = feature_extractor.predict(img_array)\n",
        "    features = features.flatten()\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386419c3",
      "metadata": {
        "papermill": {
          "duration": 0.005066,
          "end_time": "2024-11-01T13:20:32.141891",
          "exception": false,
          "start_time": "2024-11-01T13:20:32.136825",
          "status": "completed"
        },
        "tags": [],
        "id": "386419c3"
      },
      "source": [
        "3: Train the Logistic Regression Model\n",
        "\n",
        "   3.1 Create a Dataset of Extracted Features:\n",
        "\n",
        "        \n",
        "        For each image, extract its feature vector and pair it with the class label (e.g., 1 for fire/smoke and 0 for none)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f568a409",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-01T13:20:32.154353Z",
          "iopub.status.busy": "2024-11-01T13:20:32.153947Z",
          "iopub.status.idle": "2024-11-01T13:20:32.719026Z",
          "shell.execute_reply": "2024-11-01T13:20:32.717821Z"
        },
        "papermill": {
          "duration": 0.574102,
          "end_time": "2024-11-01T13:20:32.721485",
          "exception": false,
          "start_time": "2024-11-01T13:20:32.147383",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f568a409",
        "outputId": "66195f57-330b-4add-d7f8-aa6d8691fa65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FIRE-SMOKE-DATASET//Train/Fire\n",
            "900\n",
            "/content/drive/MyDrive/FIRE-SMOKE-DATASET//Train/Smoke\n",
            "900\n",
            "/content/drive/MyDrive/FIRE-SMOKE-DATASET//Train/Neutral\n",
            "928\n",
            "2728\n",
            "2728\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "img_paths = []\n",
        "labels = []\n",
        "#paths = ['Fire','Smoke','Neutral'] # Original code. I suspect you have these inside a single 'Train' folder\n",
        "paths = ['Train/Fire','Train/Smoke','Train/Neutral'] # Suggest you change it to this\n",
        "\n",
        "for path in paths:\n",
        "    PATH = \"/content/drive/MyDrive/FIRE-SMOKE-DATASET/\"\n",
        "    #PATH = PATH+path # This is redundant now\n",
        "    for dirname, _, filenames in os.walk(f'{PATH}/{path}'): # Updated this line to use correct directory\n",
        "        print(dirname)\n",
        "        print(len(filenames))\n",
        "        for filename in filenames:\n",
        "            img_path = os.path.join(dirname,filename)\n",
        "            img_paths.append(img_path)\n",
        "            if 'Fire' in path:  # Updated to match the new paths\n",
        "                labels.append(1)\n",
        "            elif 'Smoke' in path:  # Updated to match the new paths\n",
        "                labels.append(2)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "            # print(os.path.join(dirname, filename))\n",
        "print(len(img_paths))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07979e9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-01T13:20:32.756027Z",
          "iopub.status.busy": "2024-11-01T13:20:32.755651Z",
          "iopub.status.idle": "2024-11-01T13:23:58.251195Z",
          "shell.execute_reply": "2024-11-01T13:23:58.250337Z"
        },
        "papermill": {
          "duration": 205.505021,
          "end_time": "2024-11-01T13:23:58.253587",
          "exception": false,
          "start_time": "2024-11-01T13:20:32.748566",
          "status": "completed"
        },
        "tags": [],
        "id": "c07979e9"
      },
      "outputs": [],
      "source": [
        "# Assume img_paths contains paths to images and labels has corresponding labels (1 or 0)\n",
        "feature_vectors = [preprocess_and_extract(img_path) for img_path in img_paths]\n",
        "feature_vectors = np.array(feature_vectors)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train logistic regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = log_reg.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT9s16s16oR3",
        "outputId": "b1b70d3e-2e3c-4d2d-af86-706c90d9158f"
      },
      "id": "nT9s16s16oR3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess_and_extract(img_path):\n",
        "    # Load image (replace with your image loading and feature extraction logic)\n",
        "    # image = cv2.imread(img_path)\n",
        "    # ... feature extraction ...\n",
        "    # For this example, assume a random feature vector is generated\n",
        "    return np.random.rand(10)\n",
        "\n",
        "img_paths = []\n",
        "labels = []\n",
        "#paths = ['Fire','Smoke','Neutral'] # Original code. I suspect you have these inside a single 'Train' folder\n",
        "paths = ['Train/Fire','Train/Smoke','Train/Neutral'] # Suggest you change it to this\n",
        "\n",
        "for path in paths:\n",
        "    PATH = \"/content/drive/MyDrive/FIRE-SMOKE-DATASET/\"\n",
        "    #PATH = PATH+path # This is redundant now\n",
        "    # The problem was this line: os.walk(f'{PATH}/{path}')\n",
        "    # It was creating incorrect paths like '/content/drive/MyDrive/FIRE-SMOKE-DATASET//content/drive/MyDrive/FIRE-SMOKE-DATASET/Train/Fire'\n",
        "    # Change it to the following to use correct path:\n",
        "    for dirname, _, filenames in os.walk(os.path.join(PATH, path)):\n",
        "        print(dirname)\n",
        "        print(len(filenames))\n",
        "        for filename in filenames:\n",
        "            img_path = os.path.join(dirname,filename)\n",
        "            img_paths.append(img_path)\n",
        "            if 'Fire' in path:  # Updated to match the new paths\n",
        "                labels.append(1)\n",
        "            elif 'Smoke' in path:  # Updated to match the new paths\n",
        "                labels.append(2)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "            # print(os.path.join(dirname, filename))\n",
        "\n",
        "print(len(img_paths))\n",
        "print(len(labels))\n",
        "\n",
        "feature_vectors = [preprocess_and_extract(img_path) for img_path in img_paths]\n",
        "feature_vectors = np.array(feature_vectors)\n",
        "labels = np.array(labels)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train logistic regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = log_reg.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI-rQm6j3_SV",
        "outputId": "2974080d-9387-438a-d45e-1bf0aeb74aab"
      },
      "id": "zI-rQm6j3_SV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FIRE-SMOKE-DATASET/Train/Fire\n",
            "900\n",
            "/content/drive/MyDrive/FIRE-SMOKE-DATASET/Train/Smoke\n",
            "900\n",
            "/content/drive/MyDrive/FIRE-SMOKE-DATASET/Train/Neutral\n",
            "928\n",
            "2728\n",
            "2728\n",
            "Model Accuracy: 0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUcojEDd3QOA"
      },
      "id": "TUcojEDd3QOA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17ce2df2",
      "metadata": {
        "papermill": {
          "duration": 0.424958,
          "end_time": "2024-11-01T13:23:59.104455",
          "exception": false,
          "start_time": "2024-11-01T13:23:58.679497",
          "status": "completed"
        },
        "tags": [],
        "id": "17ce2df2"
      },
      "source": [
        "3.2 Train the Logistic Regression Model:\n",
        "\n",
        "    With the extracted feature vectors, train a logistic regression model on this new dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of image paths: {len(img_paths)}\")\n",
        "print(f\"Number of labels: {len(labels)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5vOGE9hm5ej",
        "outputId": "141846c9-e75c-4402-ed0c-638384ac7b1a"
      },
      "id": "q5vOGE9hm5ej",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image paths: 2728\n",
            "Number of labels: 122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3ebe08",
      "metadata": {
        "papermill": {
          "duration": 0.42651,
          "end_time": "2024-11-01T13:24:26.169167",
          "exception": false,
          "start_time": "2024-11-01T13:24:25.742657",
          "status": "completed"
        },
        "tags": [],
        "id": "db3ebe08"
      },
      "source": [
        "Step 4: Make Predictions\n",
        "\n",
        "With the trained logistic regression model, you can now make predictions on new images. Just preprocess and extract features as before, then pass these features to the logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9069c2b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-01T13:24:27.080670Z",
          "iopub.status.busy": "2024-11-01T13:24:27.079841Z",
          "iopub.status.idle": "2024-11-01T13:24:27.338857Z",
          "shell.execute_reply": "2024-11-01T13:24:27.337698Z"
        },
        "papermill": {
          "duration": 0.744774,
          "end_time": "2024-11-01T13:24:27.340807",
          "exception": false,
          "start_time": "2024-11-01T13:24:26.596033",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9069c2b5",
        "outputId": "94994695-1c75-49f9-f7fc-8534db101f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fire\n",
            "Smoke\n",
            "Smoke\n"
          ]
        }
      ],
      "source": [
        "def predict_fire_smoke(img_path):\n",
        "    features = preprocess_and_extract(img_path)\n",
        "    prediction = log_reg.predict([features])\n",
        "    # return \"Fire/Smoke Detected\" if prediction[0] == 1 else \"No Fire/Smoke Detected\"\n",
        "    if prediction == 1:\n",
        "        return \"Fire\"\n",
        "    elif prediction == 2:\n",
        "        return \"Smoke\"\n",
        "    elif prediction == 0:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Example usage\n",
        "print(predict_fire_smoke(\"/content/drive/MyDrive/FIRE-SMOKE-DATASET/Test/Fire/image_0.jpg\"))\n",
        "print(predict_fire_smoke(\"/content/drive/MyDrive/FIRE-SMOKE-DATASET/Test/Neutral/image_0.jpg\"))\n",
        "print(predict_fire_smoke(\"/content/drive/MyDrive/FIRE-SMOKE-DATASET/Test/Smoke/image_0.jpg\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac22710e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-01T09:22:56.831498Z",
          "iopub.status.busy": "2024-11-01T09:22:56.830990Z",
          "iopub.status.idle": "2024-11-01T09:22:56.840360Z",
          "shell.execute_reply": "2024-11-01T09:22:56.838761Z",
          "shell.execute_reply.started": "2024-11-01T09:22:56.831453Z"
        },
        "papermill": {
          "duration": 0.425558,
          "end_time": "2024-11-01T13:24:28.194652",
          "exception": false,
          "start_time": "2024-11-01T13:24:27.769094",
          "status": "completed"
        },
        "tags": [],
        "id": "ac22710e"
      },
      "source": [
        "##### why did i try the above method\n",
        " -->Summary -->\n",
        "\n",
        "    The pre-trained CNN extracts meaningful features from each image.\n",
        "\n",
        "    The logistic regression model, trained on these features, acts as a classifier to detect fire or smoke.\n",
        "        \n",
        "    This approach leverages the powerful feature extraction capabilities of CNNs while keeping the classification model lightweight with logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caec0040",
      "metadata": {
        "papermill": {
          "duration": 0.422745,
          "end_time": "2024-11-01T13:24:29.044739",
          "exception": false,
          "start_time": "2024-11-01T13:24:28.621994",
          "status": "completed"
        },
        "tags": [],
        "id": "caec0040"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 1031036,
          "sourceId": 1737025,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1402026,
          "sourceId": 5482910,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 257.919596,
      "end_time": "2024-11-01T13:24:32.141790",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-11-01T13:20:14.222194",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}